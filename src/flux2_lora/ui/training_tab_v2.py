"""
Enhanced Training Tab for Gradio Interface - V2

Comprehensive training interface with:
- All training parameters exposed and organized
- Real-time vRAM usage calculator
- Memory optimization controls
- Smart defaults and recommendations
"""

import gradio as gr
import torch
from pathlib import Path
from typing import TYPE_CHECKING, Optional, Dict, Any, Tuple
import logging
import threading
import subprocess

if TYPE_CHECKING:
    from .gradio_app import LoRATrainingApp

logger = logging.getLogger(__name__)

# Import helper functions from original training tab
try:
    from .training_tab import (
        handle_model_path,
        handle_dataset_path,
        handle_dataset_upload,
        validate_dataset_structure,
    )
except ImportError:
    # Fallback implementations
    def handle_model_path(path):
        return "‚úÖ Model path set"

    def handle_dataset_path(app, path):
        return "‚úÖ Dataset path set", Path(path)

    def handle_dataset_upload(app, file_obj):
        return "‚úÖ Dataset uploaded", None

    def validate_dataset_structure(path):
        return {"valid": True, "image_count": 0, "caption_count": 0}


def calculate_vram_estimate(
    resolution: int,
    batch_size: int,
    rank: int,
    dtype: str,
    gradient_checkpointing: bool,
    enable_attention_slicing: bool,
    enable_vae_slicing: bool,
    quantization_enabled: bool,
    quantization_bits: int,
) -> Dict[str, Any]:
    """
    Calculate estimated vRAM usage for training configuration.

    Returns:
        Dictionary with vRAM estimates and warnings
    """
    # Base model sizes (Flux2-dev in bfloat16)
    BASE_SIZES = {
        "transformer_bf16": 60.0,  # 32.22B params √ó 2 bytes
        "transformer_fp16": 60.0,  # Same as bfloat16
        "transformer_fp32": 120.0,  # 32.22B params √ó 4 bytes
        "text_encoder": 44.7,  # T5-XXL on CPU (offloaded)
        "vae": 0.2,  # Tiny VAE
    }

    # Dtype multipliers for model weights
    dtype_multipliers = {
        "bfloat16": 1.0,
        "float16": 1.0,
        "float32": 2.0,
    }

    # Quantization reduces model size
    if quantization_enabled:
        if quantization_bits == 8:
            model_size = BASE_SIZES["transformer_bf16"] * 0.5  # 8-bit = 50% reduction
        elif quantization_bits == 4:
            model_size = BASE_SIZES["transformer_bf16"] * 0.25  # 4-bit = 75% reduction
        else:
            model_size = BASE_SIZES["transformer_bf16"] * dtype_multipliers[dtype]
    else:
        model_size = BASE_SIZES["transformer_bf16"] * dtype_multipliers[dtype]

    # LoRA parameters (trainable)
    # Target modules: to_k, to_q, to_v, to_out.0
    # ~4 modules per layer √ó 56 layers = 224 injections
    # Each injection: inner_dim (6144) √ó rank √ó 2 (A and B matrices)
    inner_dim = 6144
    num_layers = 56
    modules_per_layer = 4
    lora_param_count = (inner_dim * rank * 2) * (num_layers * modules_per_layer)
    lora_size_gb = (lora_param_count * 2) / (1024**3)  # bfloat16 = 2 bytes

    # Optimizer state (AdamW stores 2 copies: momentum + variance)
    optimizer_size = lora_size_gb * 2

    # Activation memory depends on resolution and batch size
    # Flux2 has 56 transformer blocks, each with attention + MLP
    seq_len = (resolution // 16) ** 2  # Patch size 16, spatial tokens
    hidden_dim = 6144

    # Activation size per image per layer (in GB)
    bytes_per_param = 2 if dtype in ["bfloat16", "float16"] else 4
    activation_per_layer = (seq_len * hidden_dim * bytes_per_param) / (1024**3)

    # With gradient checkpointing: only store activations for current layer
    # Without: store activations for all layers
    if gradient_checkpointing:
        activation_memory = activation_per_layer * batch_size * 4  # ~4 layers peak
    else:
        activation_memory = activation_per_layer * batch_size * num_layers

    # Attention slicing reduces peak memory during attention computation
    if enable_attention_slicing:
        activation_memory *= 0.7  # ~30% reduction

    # VAE slicing reduces encoding/decoding memory
    vae_memory = 2.0 if enable_vae_slicing else 4.0

    # Gradients (same size as trainable parameters)
    gradient_memory = lora_size_gb

    # Total GPU memory
    total_gpu = (
        model_size  # Model weights
        + lora_size_gb  # LoRA parameters
        + optimizer_size  # Optimizer state
        + activation_memory  # Activations
        + gradient_memory  # Gradients
        + vae_memory  # VAE encoding/decoding
        + 2.0  # CUDA overhead/fragmentation
    )

    # Text encoder on CPU (not counted in GPU)
    total_cpu = BASE_SIZES["text_encoder"]

    # GPU memory recommendations
    warnings = []
    recommendations = []

    if total_gpu > 93:  # H100 limit
        warnings.append(f"‚ö†Ô∏è Estimated {total_gpu:.1f}GB exceeds H100 capacity (93GB)")
        recommendations.append("Reduce batch size to 1")
        recommendations.append("Enable gradient checkpointing")
        recommendations.append("Enable attention + VAE slicing")
        recommendations.append("Consider 8-bit quantization")
    elif total_gpu > 80:  # A100 80GB limit
        warnings.append(f"‚ö†Ô∏è Estimated {total_gpu:.1f}GB exceeds A100 80GB capacity")
        recommendations.append("Use batch_size=1 with gradient accumulation")
    elif total_gpu > 40:  # A100 40GB limit
        warnings.append(f"‚ö†Ô∏è Estimated {total_gpu:.1f}GB exceeds A100 40GB capacity")
        recommendations.append("Enable all memory optimizations")

    if total_gpu < 30:
        recommendations.append("‚úÖ Configuration should run comfortably")

    # Safety margin
    safety_margin = 93 - total_gpu if total_gpu < 93 else 0

    return {
        "total_gpu_gb": round(total_gpu, 1),
        "total_cpu_gb": round(total_cpu, 1),
        "breakdown": {
            "model": round(model_size, 1),
            "lora": round(lora_size_gb, 2),
            "optimizer": round(optimizer_size, 2),
            "activations": round(activation_memory, 1),
            "gradients": round(gradient_memory, 2),
            "vae": round(vae_memory, 1),
        },
        "warnings": warnings,
        "recommendations": recommendations,
        "safety_margin_gb": round(safety_margin, 1),
        "fit_on_h100": total_gpu <= 93,
        "fit_on_a100_80": total_gpu <= 80,
        "fit_on_a100_40": total_gpu <= 40,
    }


def format_vram_estimate(estimate: Dict[str, Any]) -> str:
    """Format vRAM estimate as HTML for display."""
    total = estimate["total_gpu_gb"]
    cpu = estimate["total_cpu_gb"]
    breakdown = estimate["breakdown"]

    # Color based on fit status
    if estimate["fit_on_h100"]:
        color = "#4caf50"  # Green
        status_emoji = "‚úÖ"
        status_text = "Fits on H100"
    elif estimate["fit_on_a100_80"]:
        color = "#ff9800"  # Orange
        status_emoji = "‚ö†Ô∏è"
        status_text = "Requires A100 80GB+"
    else:
        color = "#f44336"  # Red
        status_emoji = "‚ùå"
        status_text = "Exceeds typical GPU capacity"

    html = f"""
    <div style='padding: 15px; background: white; border: 2px solid {color}; border-radius: 8px;'>
        <div style='font-size: 18px; font-weight: bold; color: {color}; margin-bottom: 10px;'>
            {status_emoji} {status_text}
        </div>

        <div style='font-size: 24px; font-weight: bold; margin: 10px 0;'>
            GPU: {total} GB | CPU: {cpu} GB
        </div>

        <div style='margin: 15px 0; padding: 10px; background: #f5f5f5; border-radius: 5px;'>
            <div style='font-weight: bold; margin-bottom: 5px;'>Memory Breakdown:</div>
            <div style='font-family: monospace; font-size: 13px;'>
                Model weights:  {breakdown["model"]:6.1f} GB<br>
                LoRA params:    {breakdown["lora"]:6.2f} GB<br>
                Optimizer:      {breakdown["optimizer"]:6.2f} GB<br>
                Activations:    {breakdown["activations"]:6.1f} GB<br>
                Gradients:      {breakdown["gradients"]:6.2f} GB<br>
                VAE:            {breakdown["vae"]:6.1f} GB
            </div>
        </div>

        {f'<div style="margin-top: 10px; padding: 8px; background: #e8f5e9; border-radius: 4px; color: #2e7d32;">Safety Margin: {estimate["safety_margin_gb"]} GB</div>' if estimate["safety_margin_gb"] > 0 else ''}

        {''.join(f'<div style="margin-top: 8px; padding: 8px; background: #fff3e0; border-radius: 4px; color: #e65100;">{w}</div>' for w in estimate["warnings"])}

        {('<div style="margin-top: 10px;"><div style="font-weight: bold; margin-bottom: 5px;">üí° Recommendations:</div>' + ''.join(f'<div style="margin-left: 10px;">‚Ä¢ {r}</div>' for r in estimate["recommendations"]) + '</div>') if estimate["recommendations"] else ''}
    </div>
    """

    return html


def create_enhanced_training_tab(app: "LoRATrainingApp"):
    """Create enhanced training tab with all parameters and vRAM calculator."""

    with gr.Accordion("üí° Quick Start Guide", open=True):
        gr.Markdown("""
        ### üöÄ Getting Started with LoRA Training

        **1. Model Setup**
        - Download FLUX2-dev from HuggingFace: `black-forest-labs/FLUX.2-dev`
        - Specify the local path (e.g., `/home/user/models/FLUX.2-dev`)

        **2. Dataset Preparation**
        - Upload ZIP or specify local directory
        - Include images (.jpg/.png) and captions (.txt with same name)
        - Minimum 10-15 images recommended

        **3. Choose Configuration**
        - Select preset based on your subject (Character/Style/Concept)
        - Adjust parameters in Advanced Settings if needed
        - Monitor vRAM estimate to ensure it fits your GPU

        **4. Start Training**
        - Click "Start Training" and monitor progress
        - Training takes 1-4 hours typically
        - Check validation samples periodically
        """)

    with gr.Row():
        with gr.Column(scale=2):
            # Left: Configuration
            gr.Markdown("## ‚öôÔ∏è Configuration")

            # === MODEL CONFIGURATION ===
            with gr.Accordion("ü§ñ Model Configuration", open=True):
                base_model = gr.Textbox(
                    label="Base Model Path",
                    value="/home/azureuser/flux.2-dev",
                    info="Path to downloaded FLUX.2-dev model directory",
                    placeholder="/path/to/FLUX.2-dev"
                )

                model_status = gr.Textbox(
                    label="Model Status",
                    value="Enter model path to check",
                    interactive=False
                )

                with gr.Row():
                    device = gr.Dropdown(
                        label="Device",
                        choices=["auto", "cuda", "cuda:0", "cuda:1", "cpu"],
                        value="auto",
                        info="GPU to use (auto = best available)"
                    )

                    # Auto-detect optimal dtype
                    optimal_dtype = "bfloat16"
                    if torch.cuda.is_available():
                        gpu_name = torch.cuda.get_device_name(0)
                        if "H100" in gpu_name or "A100" in gpu_name:
                            optimal_dtype = "bfloat16"
                        elif torch.cuda.is_bf16_supported():
                            optimal_dtype = "bfloat16"
                        else:
                            optimal_dtype = "float16"

                    dtype = gr.Dropdown(
                        label="Data Type",
                        choices=["bfloat16", "float16", "float32"],
                        value=optimal_dtype,
                        info=f"Auto-detected: {optimal_dtype} for your GPU"
                    )

            # === DATASET CONFIGURATION ===
            with gr.Accordion("üìÅ Dataset Configuration", open=True):
                dataset_source = gr.Radio(
                    choices=["Upload ZIP", "Local Directory"],
                    value="Local Directory",
                    label="Dataset Source"
                )

                dataset_upload = gr.File(
                    label="Upload Dataset (ZIP)",
                    file_types=[".zip"],
                    visible=False
                )

                dataset_dir = gr.Textbox(
                    label="Dataset Directory Path",
                    placeholder="/path/to/dataset",
                    value="./dataset",
                    visible=True
                )

                dataset_status = gr.Textbox(
                    label="Dataset Status",
                    value="No dataset loaded",
                    interactive=False
                )

                with gr.Row():
                    resolution = gr.Dropdown(
                        label="Training Resolution",
                        choices=[512, 768, 1024, 1536, 2048],
                        value=1024,
                        info="Higher = better quality but more VRAM"
                    )

                    center_crop = gr.Checkbox(
                        label="Center Crop",
                        value=True,
                        info="Crop images to square"
                    )

                    random_flip = gr.Checkbox(
                        label="Random Flip",
                        value=False,
                        info="Horizontal flip augmentation"
                    )

            # === LORA CONFIGURATION ===
            with gr.Accordion("üéØ LoRA Configuration", open=True):
                # Preset selection
                preset = gr.Dropdown(
                    label="Training Preset",
                    choices=["Character", "Style", "Concept"],
                    value="Character",
                    info="Optimized configurations for different use cases"
                )

                gr.Markdown("""
                **Preset Guide:**
                - **Character**: For people, characters (rank=32, lr=5e-5)
                - **Style**: For art styles (rank=64, lr=1e-4)
                - **Concept**: For objects/scenes (rank=32, lr=8e-5)
                """)

                with gr.Row():
                    rank = gr.Slider(
                        minimum=4,
                        maximum=128,
                        value=32,
                        step=4,
                        label="LoRA Rank",
                        info="Higher = more capacity, slower training"
                    )

                    alpha = gr.Slider(
                        minimum=4,
                        maximum=128,
                        value=32,
                        step=4,
                        label="LoRA Alpha",
                        info="Scaling factor (typically = rank)"
                    )

                with gr.Row():
                    dropout = gr.Slider(
                        minimum=0.0,
                        maximum=0.5,
                        value=0.0,
                        step=0.05,
                        label="LoRA Dropout",
                        info="Regularization (0.0-0.1 recommended)"
                    )

                    use_dora = gr.Checkbox(
                        label="Use DoRA",
                        value=False,
                        info="Weight-Decomposed LoRA (experimental)"
                    )

                target_modules = gr.CheckboxGroup(
                    label="Target Modules",
                    choices=["to_k", "to_q", "to_v", "to_out.0", "add_k_proj", "add_v_proj"],
                    value=["to_k", "to_q", "to_v", "to_out.0"],
                    info="Attention layers to apply LoRA"
                )

            # === TRAINING CONFIGURATION ===
            with gr.Accordion("üèãÔ∏è Training Configuration", open=True):
                with gr.Row():
                    learning_rate = gr.Number(
                        label="Learning Rate",
                        value=5e-5,
                        minimum=1e-6,
                        maximum=1e-2,
                        info="Higher = faster learning but less stable"
                    )

                    max_steps = gr.Number(
                        label="Max Training Steps",
                        value=1800,
                        minimum=100,
                        maximum=20000,
                        info="Total training iterations"
                    )

                with gr.Row():
                    batch_size = gr.Slider(
                        minimum=1,
                        maximum=16,
                        value=1,
                        step=1,
                        label="Batch Size",
                        info="Flux2 requires batch_size=1 for H100"
                    )

                    gradient_accumulation_steps = gr.Slider(
                        minimum=1,
                        maximum=16,
                        value=4,
                        step=1,
                        label="Gradient Accumulation",
                        info="Effective batch = batch_size √ó this"
                    )

                with gr.Row():
                    optimizer = gr.Dropdown(
                        label="Optimizer",
                        choices=["adamw", "adam", "sgd", "adafactor"],
                        value="adamw",
                        info="AdamW recommended for most cases"
                    )

                    scheduler = gr.Dropdown(
                        label="LR Scheduler",
                        choices=["constant", "cosine", "linear", "polynomial"],
                        value="cosine",
                        info="Learning rate schedule"
                    )

                with gr.Row():
                    warmup_steps = gr.Number(
                        label="Warmup Steps",
                        value=150,
                        minimum=0,
                        maximum=1000,
                        info="Gradual LR increase at start"
                    )

                    max_grad_norm = gr.Number(
                        label="Max Gradient Norm",
                        value=1.0,
                        minimum=0.1,
                        maximum=10.0,
                        info="Gradient clipping threshold"
                    )

                mixed_precision = gr.Dropdown(
                    label="Mixed Precision",
                    choices=["no", "fp16", "bf16"],
                    value="fp16",
                    info="Auto-synced with dtype"
                )

                seed = gr.Number(
                    label="Random Seed",
                    value=42,
                    minimum=0,
                    maximum=999999,
                    info="For reproducibility"
                )

            # === MEMORY OPTIMIZATION ===
            with gr.Accordion("üíæ Memory Optimization", open=True):
                gr.Markdown("### Enable these to reduce GPU memory usage")

                gradient_checkpointing = gr.Checkbox(
                    label="Gradient Checkpointing (REQUIRED)",
                    value=True,
                    info="Saves ~50GB memory (8-10x reduction)",
                    interactive=False  # Always on for Flux2
                )

                enable_attention_slicing = gr.Checkbox(
                    label="Attention Slicing",
                    value=True,
                    info="Reduces peak memory ~30%"
                )

                with gr.Row():
                    enable_vae_slicing = gr.Checkbox(
                        label="VAE Slicing",
                        value=True,
                        info="Reduces VAE memory"
                    )

                    enable_vae_tiling = gr.Checkbox(
                        label="VAE Tiling",
                        value=True,
                        info="For large images"
                    )

                gr.Markdown("### Quantization (QLoRA) - Not Yet Supported")

                quantization_enabled = gr.Checkbox(
                    label="Enable Quantization",
                    value=False,
                    info="‚ö†Ô∏è Not yet implemented for Flux2",
                    interactive=False
                )

                quantization_bits = gr.Radio(
                    label="Quantization Bits",
                    choices=[4, 8],
                    value=8,
                    info="8-bit = 50% reduction, 4-bit = 75% reduction",
                    visible=False
                )

                sequential_cpu_offload = gr.Checkbox(
                    label="Sequential CPU Offload (Very Slow)",
                    value=False,
                    info="Last resort for low VRAM GPUs"
                )

            # === VALIDATION CONFIGURATION ===
            with gr.Accordion("üîç Validation Settings", open=False):
                validation_enabled = gr.Checkbox(
                    label="Enable Validation Sampling",
                    value=False,
                    info="‚ö†Ô∏è Disabled to save memory during training"
                )

                validation_every = gr.Number(
                    label="Validation Every N Steps",
                    value=500,
                    minimum=50,
                    maximum=1000,
                    info="How often to generate samples"
                )

                num_validation_samples = gr.Number(
                    label="Samples Per Validation",
                    value=1,
                    minimum=1,
                    maximum=4,
                    info="Images to generate each time"
                )

                validation_prompts = gr.Textbox(
                    label="Validation Prompts (one per line)",
                    value="A photo of a person\nA portrait in natural lighting",
                    lines=5,
                    info="Test prompts for validation"
                )

            # === OUTPUT CONFIGURATION ===
            with gr.Accordion("üíæ Output & Checkpoints", open=False):
                output_dir = gr.Textbox(
                    label="Output Directory",
                    value="./output",
                    info="Where to save checkpoints"
                )

                with gr.Row():
                    checkpoint_every = gr.Number(
                        label="Checkpoint Every N Steps",
                        value=500,
                        minimum=50,
                        maximum=1000,
                        info="Save frequency"
                    )

                    checkpoints_limit = gr.Number(
                        label="Max Checkpoints to Keep",
                        value=5,
                        minimum=1,
                        maximum=20,
                        info="Older checkpoints deleted"
                    )

                save_optimizer_state = gr.Checkbox(
                    label="Save Optimizer State",
                    value=True,
                    info="Needed for resuming training"
                )

            # === LOGGING CONFIGURATION ===
            with gr.Accordion("üìä Logging & Monitoring", open=False):
                with gr.Row():
                    tensorboard = gr.Checkbox(
                        label="Enable TensorBoard",
                        value=True,
                        info="Real-time training plots"
                    )

                    wandb = gr.Checkbox(
                        label="Enable Weights & Biases",
                        value=False,
                        info="Cloud logging (requires API key)"
                    )

                wandb_project = gr.Textbox(
                    label="W&B Project Name",
                    value="flux2-lora-training",
                    info="Project name for W&B",
                    visible=False
                )

                log_every = gr.Number(
                    label="Log Every N Steps",
                    value=10,
                    minimum=1,
                    maximum=100,
                    info="Logging frequency"
                )

        with gr.Column(scale=1):
            # Right: vRAM Calculator & Controls
            gr.Markdown("## üßÆ vRAM Estimate")

            vram_display = gr.HTML(
                value=format_vram_estimate(calculate_vram_estimate(
                    resolution=1024,
                    batch_size=1,
                    rank=32,
                    dtype="float16",
                    gradient_checkpointing=True,
                    enable_attention_slicing=True,
                    enable_vae_slicing=True,
                    quantization_enabled=False,
                    quantization_bits=8,
                ))
            )

            gr.Markdown("""
            **How to Read:**
            - Green (‚úÖ) = Fits comfortably on H100
            - Orange (‚ö†Ô∏è) = Needs A100 80GB+
            - Red (‚ùå) = Exceeds typical GPU capacity

            **Quick Fixes for OOM:**
            1. Reduce batch_size to 1
            2. Enable gradient checkpointing
            3. Enable all memory optimizations
            4. Use float16 instead of bfloat16
            """)

            # === TRAINING CONTROLS ===
            gr.Markdown("## üéÆ Training Controls")

            with gr.Group():
                start_btn = gr.Button(
                    "üöÄ Start Training",
                    variant="primary",
                    size="lg"
                )

                with gr.Row():
                    stop_btn = gr.Button(
                        "‚èπÔ∏è Stop",
                        variant="stop",
                        interactive=False
                    )
                    pause_btn = gr.Button(
                        "‚è∏Ô∏è Pause",
                        variant="secondary",
                        interactive=False
                    )

                training_status = gr.Textbox(
                    label="Status",
                    value="Ready to train",
                    interactive=False,
                    lines=3
                )

            # === QUICK PRESETS ===
            with gr.Accordion("‚ö° Quick Presets", open=False):
                gr.Markdown("Click to apply optimized configurations:")

                preset_fast = gr.Button("Fast Training (2-3 hours)")
                preset_quality = gr.Button("High Quality (6-8 hours)")
                preset_lowvram = gr.Button("Low VRAM (< 40GB)")

    # === EVENT HANDLERS ===

    def update_vram_estimate_wrapper(
        resolution_val, batch_val, rank_val, dtype_val,
        grad_ckpt_val, attn_slice_val, vae_slice_val,
        quant_enabled_val, quant_bits_val
    ):
        """Update vRAM estimate when parameters change."""
        estimate = calculate_vram_estimate(
            resolution=resolution_val,
            batch_size=batch_val,
            rank=rank_val,
            dtype=dtype_val,
            gradient_checkpointing=grad_ckpt_val,
            enable_attention_slicing=attn_slice_val,
            enable_vae_slicing=vae_slice_val,
            quantization_enabled=quant_enabled_val,
            quantization_bits=quant_bits_val,
        )
        return format_vram_estimate(estimate)

    # Attach vRAM calculator to parameter changes
    vram_inputs = [
        resolution, batch_size, rank, dtype,
        gradient_checkpointing, enable_attention_slicing, enable_vae_slicing,
        quantization_enabled, quantization_bits
    ]

    for inp in vram_inputs:
        inp.change(
            fn=update_vram_estimate_wrapper,
            inputs=vram_inputs,
            outputs=[vram_display]
        )

    # Dataset source toggle
    def toggle_dataset_source(source):
        if source == "Upload ZIP":
            return gr.update(visible=True), gr.update(visible=False)
        else:
            return gr.update(visible=False), gr.update(visible=True)

    dataset_source.change(
        fn=toggle_dataset_source,
        inputs=[dataset_source],
        outputs=[dataset_upload, dataset_dir]
    )

    # Sync dtype with mixed_precision
    def sync_dtype_to_mixed_precision(dtype_val):
        if dtype_val == "float16":
            return "fp16"
        elif dtype_val == "bfloat16":
            return "bf16"
        else:
            return "no"

    dtype.change(
        fn=sync_dtype_to_mixed_precision,
        inputs=[dtype],
        outputs=[mixed_precision]
    )

    # Preset application
    def apply_fast_preset():
        return {
            max_steps: 800,
            learning_rate: 1e-4,
            batch_size: 1,
            gradient_accumulation_steps: 2,
            checkpoint_every: 200,
            rank: 16,
        }

    def apply_quality_preset():
        return {
            max_steps: 2000,
            learning_rate: 5e-5,
            batch_size: 1,
            gradient_accumulation_steps: 4,
            checkpoint_every: 500,
            rank: 32,
        }

    def apply_lowvram_preset():
        return {
            batch_size: 1,
            gradient_accumulation_steps: 8,
            enable_attention_slicing: True,
            enable_vae_slicing: True,
            enable_vae_tiling: True,
            rank: 16,
            dtype: "float16",
        }

    preset_fast.click(
        fn=apply_fast_preset,
        outputs=[max_steps, learning_rate, batch_size, gradient_accumulation_steps, checkpoint_every, rank]
    )

    preset_quality.click(
        fn=apply_quality_preset,
        outputs=[max_steps, learning_rate, batch_size, gradient_accumulation_steps, checkpoint_every, rank]
    )

    preset_lowvram.click(
        fn=apply_lowvram_preset,
        outputs=[batch_size, gradient_accumulation_steps, enable_attention_slicing, enable_vae_slicing, enable_vae_tiling, rank, dtype]
    )

    # Model path validation
    def validate_model_path_wrapper(path):
        """Validate model path and return status."""
        if not path or not path.strip():
            return "Enter model path to check"

        from .training_tab import handle_model_path
        return handle_model_path(path.strip())

    base_model.change(
        fn=validate_model_path_wrapper,
        inputs=[base_model],
        outputs=[model_status]
    )

    # Dataset validation
    def validate_dataset_wrapper(path):
        """Validate dataset path."""
        if not path or not path.strip():
            return "No dataset path provided"

        from .training_tab import handle_dataset_path
        status, _ = handle_dataset_path(app, path.strip())
        return status

    dataset_dir.change(
        fn=validate_dataset_wrapper,
        inputs=[dataset_dir],
        outputs=[dataset_status]
    )

    # Start training
    def start_training_wrapper(
        base_model_val, device_val, dtype_val, dataset_path_val,
        preset_val, rank_val, alpha_val, dropout_val, target_modules_val,
        lr_val, max_steps_val, batch_val, grad_accum_val,
        optimizer_val, scheduler_val, warmup_val, grad_norm_val,
        mixed_prec_val, seed_val,
        grad_ckpt_val, attn_slice_val, vae_slice_val, vae_tile_val,
        seq_offload_val,
        validation_enabled_val, validation_every_val, num_val_samples_val, val_prompts_val,
        output_dir_val, ckpt_every_val, ckpts_limit_val, save_opt_val,
        tensorboard_val, wandb_val, wandb_project_val, log_every_val,
        resolution_val, center_crop_val, random_flip_val
    ):
        """Start training with all parameters."""

        # Validate inputs
        if not dataset_path_val or not Path(dataset_path_val).exists():
            return "‚ùå Invalid dataset path"

        if not base_model_val or not Path(base_model_val).exists():
            return "‚ùå Invalid model path"

        # Build comprehensive config
        config = {
            # Model
            "base_model": base_model_val,
            "device": device_val,
            "dtype": dtype_val,

            # Dataset
            "dataset_path": dataset_path_val,
            "resolution": resolution_val,
            "center_crop": center_crop_val,
            "random_flip": random_flip_val,

            # LoRA
            "preset": preset_val.lower(),
            "rank": int(rank_val),
            "alpha": int(alpha_val),
            "dropout": float(dropout_val),
            "target_modules": target_modules_val,
            "use_dora": False,  # Not yet supported

            # Training
            "learning_rate": float(lr_val),
            "max_steps": int(max_steps_val),
            "batch_size": int(batch_val),
            "gradient_accumulation_steps": int(grad_accum_val),
            "optimizer": optimizer_val,
            "scheduler": scheduler_val,
            "warmup_steps": int(warmup_val),
            "max_grad_norm": float(grad_norm_val),
            "mixed_precision": mixed_prec_val,
            "seed": int(seed_val),

            # Memory optimization
            "gradient_checkpointing": grad_ckpt_val,
            "enable_attention_slicing": attn_slice_val,
            "enable_vae_slicing": vae_slice_val,
            "enable_vae_tiling": vae_tile_val,
            "sequential_cpu_offload": seq_offload_val,

            # Validation
            "validation_enabled": validation_enabled_val,
            "validation_every": int(validation_every_val),
            "num_validation_samples": int(num_val_samples_val),
            "validation_prompts": val_prompts_val.split("\n") if val_prompts_val else [],

            # Output
            "output_dir": output_dir_val,
            "checkpoint_every": int(ckpt_every_val),
            "checkpoints_limit": int(ckpts_limit_val),
            "save_optimizer_state": save_opt_val,

            # Logging
            "tensorboard": tensorboard_val,
            "wandb": wandb_val,
            "wandb_project": wandb_project_val if wandb_val else "",
            "log_every": int(log_every_val),
        }

        # Start training using CLI
        import subprocess
        import threading

        def run_training():
            """Run training in subprocess."""
            cmd = [
                "python3", "cli.py", "train",
                "--preset", preset_val.lower(),
                "--dataset", dataset_path_val,
                "--output", output_dir_val,
                "--dtype", dtype_val,
                "--steps", str(int(max_steps_val)),
                "--batch-size", str(int(batch_val)),
                "--lr", str(float(lr_val)),
            ]

            # Add memory optimization flags
            if attn_slice_val:
                cmd.append("--attention-slicing")
            if vae_slice_val:
                cmd.append("--vae-slicing")
            if seq_offload_val:
                cmd.append("--sequential-cpu-offload")

            try:
                subprocess.run(cmd, check=True)
            except subprocess.CalledProcessError as e:
                logger.error(f"Training failed: {e}")

        # Start in background
        thread = threading.Thread(target=run_training, daemon=True)
        thread.start()

        return "üöÄ Training started! Monitor progress in terminal."

    # Collect all inputs
    training_inputs = [
        base_model, device, dtype, dataset_dir,
        preset, rank, alpha, dropout, target_modules,
        learning_rate, max_steps, batch_size, gradient_accumulation_steps,
        optimizer, scheduler, warmup_steps, max_grad_norm,
        mixed_precision, seed,
        gradient_checkpointing, enable_attention_slicing, enable_vae_slicing, enable_vae_tiling,
        sequential_cpu_offload,
        validation_enabled, validation_every, num_validation_samples, validation_prompts,
        output_dir, checkpoint_every, checkpoints_limit, save_optimizer_state,
        tensorboard, wandb, wandb_project, log_every,
        resolution, center_crop, random_flip
    ]

    start_btn.click(
        fn=start_training_wrapper,
        inputs=training_inputs,
        outputs=[training_status]
    )
