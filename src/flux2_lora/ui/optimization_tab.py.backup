"""
Optimization tab for the Gradio interface.

Provides hyperparameter optimization capabilities using Optuna.
"""

import os
import json
import tempfile
import threading
import time
from pathlib import Path
from typing import TYPE_CHECKING, Dict, Any, Optional

import gradio as gr

if TYPE_CHECKING:
    from .gradio_app import LoRATrainingApp

from .help_utils import help_system


def opt_generate_recommendations(results: Dict[str, Any], n_trials: int) -> str:
    """Generate personalized recommendations based on optimization results."""
    best_score = results.get("best_score", 0)

    if best_score >= 0.8:
        return "üéâ **Excellent results!** Your optimization found very high-quality parameters. Use the best_config.yaml for production training."
    elif best_score >= 0.6:
        return "üëç **Good results!** The optimized parameters should provide noticeable improvements. Download and use best_config.yaml."
    elif best_score >= 0.4:
        return "ü§î **Fair results.** Consider improving your dataset quality before production training."
    else:
        return "‚ö†Ô∏è **Poor results.** Check dataset quality and consider different optimization settings."


def create_optimization_tab(app: "LoRATrainingApp"):
    """
    Create the optimization tab interface.

    Args:
        app: Main application instance
    """
    # Help section (collapsible)
    with gr.Accordion("üí° Optimization Help & Tips", open=False):
        gr.Markdown("""
        ## ‚ö° Hyperparameter Optimization

        ### What is Hyperparameter Optimization?
        Automatically find the best settings for your LoRA training by testing different combinations of:
        - LoRA rank and alpha (model capacity)
        - Learning rate (training speed)
        - Batch size and gradient accumulation (GPU memory usage)

        ### How It Works
        1. **Bayesian Optimization**: Uses Optuna to intelligently search the parameter space
        2. **Trial Evaluation**: Each trial trains a small model and measures quality
        3. **Progressive Refinement**: Learns from past trials to suggest better parameters
        4. **Early Stopping**: Stops poor trials early to save time

        ### What Gets Optimized
        - **LoRA Rank**: 4-128 (higher = more capacity, slower training)
        - **LoRA Alpha**: 4-128 (scaling factor for LoRA weights)
        - **Learning Rate**: 1e-6 to 1e-2 (how fast the model learns)
        - **Batch Size**: 1, 2, 4, 8, 16 (images processed simultaneously)
        - **Gradient Accumulation**: 1, 2, 4, 8 (effective batch size)

        ### Expected Duration
        - **20 trials**: 4-8 hours (good for testing)
        - **50 trials**: 10-20 hours (recommended for production)
        - **100 trials**: 20-40 hours (maximum optimization)

        ### Output Files
        - `best_config.yaml`: Optimal configuration for production training
        - `optimization_results.json`: Complete optimization summary
        - `trials_data.json`: Detailed results from all trials
        """)
        - **Batch Size**: 1, 2, 4, 8, 16 (images processed simultaneously)
        - **Gradient Accumulation**: 1, 2, 4, 8 (effective batch size)

        ### Expected Results
        - **Quality Improvement**: 10-30% better final LoRA quality
        - **Time Savings**: Optimized settings train faster and more reliably
        - **Memory Efficiency**: Better GPU memory utilization

        ### Best Practices
        - **Start Small**: Use 20-30 trials for initial optimization
        - **Good Dataset**: Optimization works best with representative data
        - **Monitor Progress**: Check trial results to understand parameter effects
        - **Production Training**: Use optimized settings for final training

        ### Time Estimates
        - **20 trials**: 4-8 hours (good for development)
        - **50 trials**: 10-20 hours (recommended for production)
        - **100 trials**: 20-40 hours (maximum optimization)

        ### Output Files
        - `best_config.yaml`: Ready-to-use configuration for training
        - `optimization_results.json`: Complete optimization summary
        - `trials_data.json`: Detailed results from each trial
        """)

    with gr.Row():
        with gr.Column(scale=1):
            # Left column: Optimization setup
            gr.Markdown("## ‚öôÔ∏è Optimization Setup")

            with gr.Group():
                gr.Markdown("### Dataset")

                # Dataset selection
                opt_dataset_source = gr.Radio(
                    choices=["Upload ZIP", "Local Directory"],
                    value="Upload ZIP",
                    label="Dataset Source",
                    info="Dataset to use for optimization trials",
                )

                opt_dataset_upload = gr.File(
                    label="Upload Dataset (ZIP)",
                    file_types=[".zip"],
                    visible=True,
                    info="Upload a ZIP file containing your training dataset",
                )

                opt_dataset_path = gr.Textbox(
                    label="Dataset Directory Path",
                    placeholder="/path/to/dataset",
                    visible=False,
                    info="Local path to your dataset directory",
                )

                opt_dataset_status = gr.Textbox(
                    label="Dataset Status",
                    value="No dataset selected",
                    interactive=False,
                    info="Shows validation status of selected dataset",
                )

                gr.Markdown("### Optimization Settings")

                # Optimization parameters
                opt_trials = gr.Slider(
                    minimum=5,
                    maximum=100,
                    value=30,
                    step=5,
                    label="Number of Trials",
                    info="How many different parameter combinations to test. More trials = better results but longer time. 20-50 trials recommended.",
                )

                opt_max_steps = gr.Slider(
                    minimum=200,
                    maximum=1000,
                    value=500,
                    step=50,
                    label="Steps per Trial",
                    info="Training steps for each optimization trial. Shorter = faster optimization but potentially less accurate results.",
                )

                opt_timeout = gr.Number(
                    label="Timeout (hours)",
                    value=None,
                    minimum=0.1,
                    info="Optional: Automatically stop optimization after this many hours to prevent runaway processes",
                )

                opt_study_name = gr.Textbox(
                    label="Study Name",
                    value="flux2_lora_optimization",
                    info="Unique name for this optimization study. Used for saving/loading progress across sessions.",
                )

                # Output settings
                opt_output_dir = gr.Textbox(
                    label="Output Directory",
                    value="./optimization_results",
                    info="Directory where optimization results, trial data, and best configuration will be saved",
                )

                # Preset configurations
                with gr.Accordion("üéØ Quick Presets", open=False):
                    gr.Markdown("""
                    ### Optimization Presets

                    **Quick Test (20 trials, 4-8 hours):**
                    - Good for initial testing and development
                    - Provides decent optimization results
                    - Fast enough for iteration

                    **Standard (50 trials, 10-20 hours):**
                    - Recommended for production optimization
                    - Balances time and quality improvement
                    - Should improve results by 15-25%

                    **Maximum (100 trials, 20-40 hours):**
                    - For when you want the absolute best results
                    - Diminishing returns after 50-60 trials
                    - Only recommended for critical applications

                    **Custom:**
                    - Set your own parameters based on your needs
                    - Consider your time constraints and dataset size
                    """)

                    # Preset buttons
                    with gr.Row():
                        quick_preset = gr.Button("‚ö° Quick (20 trials)", size="sm")
                        standard_preset = gr.Button("üéØ Standard (50 trials)", size="sm")
                        max_preset = gr.Button("üèÜ Maximum (100 trials)", size="sm")

                gr.Markdown("### Optimization Controls")

                # Control buttons
                opt_start_btn = gr.Button(
                    "üöÄ Start Optimization",
                    variant="primary",
                    size="lg",
                    info="Begin hyperparameter optimization (may take several hours)",
                )

                opt_stop_btn = gr.Button(
                    "‚èπÔ∏è Stop Optimization",
                    variant="stop",
                    interactive=False,
                    info="Stop the current optimization process",
                )

                # Status
                opt_status = gr.Textbox(
                    label="Optimization Status",
                    value="Ready to optimize",
                    interactive=False,
                    lines=3,
                    info="Current optimization progress and status",
                )

        with gr.Column(scale=2):
            # Right column: Results and monitoring
            gr.Markdown("## üìä Optimization Progress")

            with gr.Group():
                # Progress visualization
                opt_progress_bar = gr.Slider(
                    minimum=0,
                    maximum=100,
                    value=0,
                    label="Optimization Progress (%)",
                    interactive=False,
                    info="Overall completion percentage of optimization",
                )

                opt_current_trial = gr.Textbox(
                    label="Current Trial",
                    value="0 / 0",
                    interactive=False,
                    info="Current trial number and total trials",
                )

                opt_best_score = gr.Textbox(
                    label="Best Score Found",
                    value="No results yet",
                    interactive=False,
                    info="Highest quality score achieved so far",
                )

                # Optimization history plot
                opt_history_plot = gr.LinePlot(
                    label="Optimization History",
                    x="Trial",
                    y="Score",
                    title="Quality Score vs Trial Number",
                    info="Shows how optimization quality improves over trials",
                )

            # Results section
            gr.Markdown("### üìã Optimization Results")

            with gr.Tabs():
                with gr.TabItem("üèÜ Best Parameters"):
                    opt_best_params = gr.JSON(
                        label="Best Hyperparameters",
                        value={},
                        info="Optimal parameter combination found by the optimization process",
                    )

                    # Download button for best config
                    download_best_config = gr.File(
                        label="Download Best Config",
                        file_count="single",
                        visible=False,
                        info="Download the optimized configuration as a YAML file for production training",
                    )

                    download_btn = gr.Button(
                        "üì• Download Best Config",
                        variant="secondary",
                        info="Download the best_config.yaml file for use in production training",
                    )

                with gr.TabItem("üìä Trial History"):
                    opt_trial_history = gr.Dataframe(
                        headers=["Trial", "Score", "Rank", "Alpha", "LR", "Batch Size", "Grad Acc"],
                        value=[],
                        datatype=["number", "number", "number", "number", "number", "number", "number"],
                        label="Trial Results",
                        info="Detailed results from all optimization trials. Higher scores are better.",
                    )

                    # Trial analysis
                    with gr.Accordion("üìà Trial Analysis", open=False):
                        gr.Markdown("""
                        ### Understanding Trial Results

                        **Score**: Quality metric combining CLIP similarity, diversity, and overfitting resistance (0-1 scale)

                        **Rank**: LoRA rank (4-128) - higher values = more model capacity
                        **Alpha**: LoRA alpha scaling factor - usually matches rank
                        **LR**: Learning rate in scientific notation (e.g., 1.23e-04 = 0.000123)
                        **Batch Size**: Images processed simultaneously
                        **Grad Acc**: Gradient accumulation steps (effective batch size multiplier)

                        ### Interpreting Results
                        - **Top trials**: Focus on the highest-scoring parameter combinations
                        - **Parameter patterns**: Look for consistent values across good trials
                        - **Trade-offs**: Higher rank = better quality but slower training
                        - **Stability**: Good parameters should work consistently across trials
                        """)

                with gr.TabItem("üìà Parameter Importance"):
                    opt_param_importance = gr.Plot(
                        label="Parameter Importance",
                        info="Shows which parameters had the most impact on optimization results",
                    )

                    importance_explanation = gr.Markdown("""
                    ### Parameter Importance Analysis

                    This chart shows which hyperparameters had the most influence on the final optimization results.
                    Higher bars indicate parameters that significantly affected the quality score.

                    **Key Insights:**
                    - **Learning Rate**: Often the most important parameter
                    - **LoRA Rank**: Critical for model capacity vs. training speed
                    - **Batch Size**: Important for GPU memory utilization
                    - **Alpha**: Usually correlates with rank
                    - **Gradient Accumulation**: Less critical unless memory-constrained

                    Use this information to focus your manual tuning efforts on the most impactful parameters.
                    """)

                with gr.TabItem("üí° Recommendations"):
                    opt_recommendations = gr.Markdown(
                        value="No optimization completed yet. Run optimization to see recommendations.",
                        info="Personalized recommendations based on your optimization results",
                    )

                with gr.TabItem("üìä Trial History"):
                    opt_trial_history = gr.Dataframe(
                        headers=["Trial", "Score", "Rank", "Alpha", "LR", "Batch Size", "Grad Acc"],
                        value=[],
                        datatype=[
                            "number",
                            "number",
                            "number",
                            "number",
                            "number",
                            "number",
                            "number",
                        ],
                        label="Trial Results",
                        info="Detailed results from all optimization trials",
                    )

                with gr.TabItem("üìà Parameter Importance"):
                    opt_param_importance = gr.Plot(
                        label="Parameter Importance",
                        info="Shows which parameters had the most impact on results",
                    )

    # State variables
    opt_active = gr.State(False)
    opt_results = gr.State({})
    opt_dataset_path = gr.State(None)
    opt_config_file = gr.State(None)

    # Preset handlers
    def apply_quick_preset():
        """Apply quick optimization preset."""
        return {
            opt_trials: 20,
            opt_max_steps: 300,
            opt_timeout: 8.0,
        }

    def apply_standard_preset():
        """Apply standard optimization preset."""
        return {
            opt_trials: 50,
            opt_max_steps: 500,
            opt_timeout: None,
        }

    def apply_max_preset():
        """Apply maximum optimization preset."""
        return {
            opt_trials: 100,
            opt_max_steps: 500,
            opt_timeout: None,
        }

    quick_preset.click(
        fn=apply_quick_preset,
        outputs=[opt_trials, opt_max_steps, opt_timeout],
    )

    standard_preset.click(
        fn=apply_standard_preset,
        outputs=[opt_trials, opt_max_steps, opt_timeout],
    )

    max_preset.click(
        fn=apply_max_preset,
        outputs=[opt_trials, opt_max_steps, opt_timeout],
    )

    # Event handlers
    def opt_update_dataset_visibility(source):
        """Update dataset input visibility."""
        if source == "Upload ZIP":
            return gr.update(visible=True), gr.update(visible=False)
        else:
            return gr.update(visible=False), gr.update(visible=True)

    opt_dataset_source.change(
        fn=opt_update_dataset_visibility,
        inputs=[opt_dataset_source],
        outputs=[opt_dataset_upload, opt_dataset_path],
    )

    # Dataset upload handler
    def opt_handle_dataset_upload(file_obj):
        """Handle dataset upload for optimization."""
        if file_obj:
            from .training_tab import handle_dataset_upload

            status, path = handle_dataset_upload(app, file_obj)
            return status, path
        return "No file uploaded", None

    opt_dataset_upload.change(
        fn=opt_handle_dataset_upload,
        inputs=[opt_dataset_upload],
        outputs=[opt_dataset_status, opt_dataset_path],
    )

    # Dataset path handler
    def opt_handle_dataset_path(path):
        """Handle dataset path input."""
        if path and path.strip():
            from .training_tab import handle_dataset_path

            status, dataset_path = handle_dataset_path(app, path.strip())
            return status
        return "No dataset path provided"

    opt_dataset_path.change(
        fn=opt_handle_dataset_path,
        inputs=[opt_dataset_path],
        outputs=[opt_dataset_status],
    )

    # Start optimization handler
    def opt_start_optimization_handler(
        trials, max_steps, timeout, study_name, output_dir,
        opt_active, opt_results, opt_dataset_path
    ):
        """Handle optimization start."""
        if opt_active:
            return "Optimization is already running", opt_active, opt_results

        if not opt_dataset_path:
            return "‚ùå No dataset selected. Please upload or specify a dataset path.", opt_active, opt_results

        try:
            # Check if Optuna is available
            import optuna
        except ImportError:
            return "‚ùå Optuna is required for optimization. Install with: pip install optuna", opt_active, opt_results

        # Validate output directory
        output_path = Path(output_dir)
        try:
            output_path.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            return f"‚ùå Cannot create output directory: {e}", opt_active, opt_results

        # Start optimization in background
        def optimization_thread():
            """Run optimization in background thread."""
            try:
                from flux2_lora.optimization import create_optimizer

                app.update_training_state("opt_status", f"üöÄ Starting optimization with {trials} trials...")

                # Create optimizer
                optimizer = create_optimizer(
                    n_trials=int(trials),
                    dataset_path=opt_dataset_path,
                    output_dir=output_dir,
                    timeout_hours=timeout if timeout and timeout > 0 else None,
                    max_steps=int(max_steps)
                )

                # Progress callback
                def progress_callback(trial_number, total_trials, score=None, params=None):
                    """Update progress in UI."""
                    progress = (trial_number / total_trials) * 100
                    app.update_training_state("opt_progress", progress)
                    app.update_training_state("opt_current_trial", f"{trial_number} / {total_trials}")

                    if score is not None:
                        app.update_training_state("opt_best_score", ".4f")

                    # Update trial history (simplified for UI)
                    if params:
                        app.update_training_state("opt_latest_params", params)

                    # Update status with progress
                    app.update_training_state("opt_status",
                        f"üîÑ Running trial {trial_number}/{total_trials} | "
                        f"Progress: {progress:.1f}% | "
                        f"Best score: {app.get_training_state('opt_best_score', 'N/A')}")

                # Run optimization (this will be implemented with progress callbacks)
                results = optimizer.optimize(
                    dataset_path=opt_dataset_path,
                    study_name=study_name
                )

                # Generate recommendations
                recommendations = opt_generate_recommendations(results, trials)

                app.update_training_state("opt_results", results)
                app.update_training_state("opt_recommendations", recommendations)
                app.update_training_state("opt_status",
                    f"‚úÖ Optimization completed! Best score: {results['best_score']:.4f}")

                # Set config file path for download
                config_path = Path(output_dir) / "best_config.yaml"
                if config_path.exists():
                    app.update_training_state("opt_config_file", str(config_path))

            except Exception as e:
                app.update_training_state("opt_status", f"‚ùå Optimization failed: {str(e)}")
            finally:
                app.update_training_state("opt_active", False)

        # Start thread
        thread = threading.Thread(target=optimization_thread, daemon=True)
        thread.start()

        return f"üöÄ Optimization started with {trials} trials! This may take several hours.", True, {}

    opt_start_btn.click(
        fn=opt_start_optimization_handler,
        inputs=[
            opt_trials,
            opt_max_steps,
            opt_timeout,
            opt_study_name,
            opt_output_dir,
            opt_active,
            opt_results,
            opt_dataset_path,
        ],
        outputs=[opt_status, opt_active, opt_results],
    )

    # Stop optimization handler
    def opt_stop_optimization_handler(opt_active):
        """Handle optimization stop."""
        if not opt_active:
            return "No optimization is currently running", opt_active

        # Set stop flag (implement in optimizer)
        app.update_training_state("opt_should_stop", True)
        return "‚èπÔ∏è Stopping optimization...", opt_active

    opt_stop_btn.click(
        fn=opt_stop_optimization_handler,
        inputs=[opt_active],
        outputs=[opt_status, opt_active],
    )

    # Download handler
    def opt_download_config(opt_config_file):
        """Handle config file download."""
        if opt_config_file and Path(opt_config_file).exists():
            return opt_config_file
        return None

    download_btn.click(
        fn=opt_download_config,
        inputs=[opt_config_file],
        outputs=[download_best_config],
    )

    # Update UI periodically
    def opt_update_ui():
        opt_active = app.get_training_state("opt_active", False)
        progress = app.get_training_state("opt_progress", 0.0)
        current_trial = app.get_training_state("opt_current_trial", "0 / 0")
        best_score = app.get_training_state("opt_best_score", "No results yet")
        status = app.get_training_state("opt_status", "Ready to optimize")
        results = app.get_training_state("opt_results", {})
        recommendations = app.get_training_state("opt_recommendations", "No optimization completed yet. Run optimization to see recommendations.")
        config_file = app.get_training_state("opt_config_file")

        # Prepare history data (simplified)
        history_data = []
        if results and "trials_data" in results:
            # Extract trial data for the table
            for trial in results["trials_data"][:20]:  # Show first 20 trials
                if trial.get("state") == "COMPLETE":
                    params = trial.get("params", {})
                    history_data.append([
                        trial.get("number", 0),
                        trial.get("value", 0),
                        params.get("rank", 0),
                        params.get("alpha", 0),
                        params.get("learning_rate", 0),
                        params.get("batch_size", 0),
                        params.get("gradient_accumulation", 0),
                    ])

        # Update recommendations display
        if recommendations != "No optimization completed yet. Run optimization to see recommendations.":
            recommendations_display = recommendations
        else:
            recommendations_display = recommendations

        return (
            progress,
            current_trial,
            best_score,
            status,
            results.get("best_params", {}),
            history_data,
            recommendations_display,
            config_file if config_file and Path(config_file).exists() else None,
            gr.update(interactive=not opt_active),
            gr.update(interactive=opt_active),
            gr.update(visible=bool(config_file and Path(config_file).exists())),
        )

    # Set up periodic UI updates
    opt_timer = gr.Timer(2.0)  # Update every 2 seconds
    opt_timer.tick(
        fn=opt_update_ui,
        outputs=[
            opt_progress_bar,
            opt_current_trial,
            opt_best_score,
            opt_status,
            opt_best_params,
            opt_trial_history,
            opt_recommendations,
            opt_config_file,
            opt_start_btn,
            opt_stop_btn,
            download_best_config,
        ],
    )
